# transformers
## Comparing industrial implementation of transformers with my own implementation
My implementation is based on the original "Attention is all you need" paper. Link: https://arxiv.org/pdf/1706.03762.pdf  
Other resources used:  
http://nlp.seas.harvard.edu/annotated-transformer/   
http://jalammar.github.io/illustrated-transformer/  
Attention:  
https://machinelearningmastery.com/the-attention-mechanism-from-scratch/  

